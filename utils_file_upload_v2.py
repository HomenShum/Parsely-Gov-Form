"""
Optimized file upload and processing utility for the Parsely project.
This module provides a modular and efficient approach to handling different types of file uploads
and their processing using modern Python practices.
"""

import logging
import asyncio
import aiohttp
import json
import streamlit as st
from typing import List, Dict, Any, Optional, Union, Tuple
from pathlib import Path
from tempfile import NamedTemporaryFile
from qdrant_client import QdrantClient, AsyncQdrantClient, models
from llama_index.core.schema import Document
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.core import VectorStoreIndex, StorageContext, Settings
from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding
from llama_index.embeddings.openai import OpenAIEmbedding
from pydantic import BaseModel, Field
from enum import Enum
from dataclasses import dataclass
import os
from llama_parse import LlamaParse
from pydantic_ai import Agent, RunContext
import datetime
from typing import Tuple
import nest_asyncio
from openai import AsyncOpenAI
import hashlib
import pandas as pd

# Apply nest_asyncio to handle nested event loops
nest_asyncio.apply()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Initialize OpenAI client in session state
if "openai_client" not in st.session_state:
    st.session_state.openai_client = AsyncOpenAI(
        api_key=st.secrets.get("OPENAI_API_KEY")
    )

class ProcessingMethod(str, Enum):
    """Processing methods for different document types"""
    LLAMA_PARSER = "llama_parser"  # Precision parsing for complex documents
    PARSE_API_URL = "parse_api_url"  # General parsing for simple documents
    COLPALI = "colpali"  # Vision-based parsing for images and diagrams

@dataclass
class ProcessingConfig:
    """Configuration for document processing"""
    azure_openai_key: Optional[str] = None
    openai_key: Optional[str] = None
    azure_endpoint: Optional[str] = None
    embedding_model: str = "text-embedding-3-small"
    processing_method: ProcessingMethod = ProcessingMethod.PARSE_API_URL

class AgentMetadata(BaseModel):
    """Metadata generated by the agent."""
    title: str
    hashtags: List[str] = Field(description="List of hashtags for the document")
    hypothetical_questions: List[str]
    summary: str

class DocumentInfo(BaseModel):
    """Complete document information combining external and agent-generated data."""
    source_name: str
    index: int
    text_chunk: str
    title: str
    hashtags: List[str]
    hypothetical_questions: List[str]
    summary: str
    metadata: Dict[str, Any]

class ProcessingResult(BaseModel):
    """Processing result with error handling"""
    success: bool
    message: str
    method_used: ProcessingMethod
    document_info: Optional[DocumentInfo] = None
    error: Optional[str] = None

class MethodRecommendationInput(BaseModel):
    user_complexity_preference: str = Field(
        ..., 
        description="User's stated complexity and requirements for the PDF document"
    )

class MethodRecommendationOutput(BaseModel):
    recommended_method: ProcessingMethod = Field(
        ..., 
        description="Recommended processing method"
    )
    explanation: str = Field(
        ..., 
        description="Reason for this recommendation"
    )


#########################################################
# Document Processing Method Recommendation Agents Based on Complexity and Needs
#########################################################

# Initialize Method Recommendation Agent
method_recommendation_agent = Agent(
    model="openai:gpt-4o-mini",
    deps_type=MethodRecommendationInput,
    result_type=MethodRecommendationOutput,
    system_prompt="""You are a document processing method recommendation agent. Your task is to recommend the best processing method based on the document complexity provided by the user.

                    For simple text-based documents, recommend PARSE_API_URL.
                    For complex documents without images/diagrams, recommend LLAMA_PARSER.
                    For documents with images/diagrams, recommend COLPALI.

                    Provide clear explanations for your recommendations."""
                    )

@method_recommendation_agent.tool
async def get_method_recommendation(search_context: RunContext[MethodRecommendationInput]) -> MethodRecommendationOutput:
    """Get document processing method recommendation based on user input"""
    try:
        complexity = search_context.deps.user_complexity_preference.strip().lower()
        logger.info(f"Processing complexity: {complexity}")

        if "simple" in complexity:
            logger.info("Recommending PARSE_API_URL for simple document")
            return MethodRecommendationOutput(
                recommended_method=ProcessingMethod.PARSE_API_URL,
                explanation="For simple text-based documents, we use the basic parsing API which is fast and efficient for straightforward content extraction."
            )
        elif "images" in complexity or "diagrams" in complexity:
            logger.info("Recommending COLPALI for document with images/diagrams")
            return MethodRecommendationOutput(
                recommended_method=ProcessingMethod.COLPALI,
                explanation="For documents containing images or diagrams, we use COLPALI which provides advanced vision capabilities for comprehensive document analysis."
            )
        elif "complex" in complexity:
            logger.info("Recommending LLAMA_PARSER for complex document")
            return MethodRecommendationOutput(
                recommended_method=ProcessingMethod.LLAMA_PARSER,
                explanation="For complex documents without images, we use the LLAMA parser which excels at handling intricate document structures and relationships."
            )
        else:
            logger.warning(f"No clear match for complexity: {complexity}, defaulting to PARSE_API_URL")
            return MethodRecommendationOutput(
                recommended_method=ProcessingMethod.PARSE_API_URL,
                explanation="No specific complexity requirements detected, using the basic parser for general document processing."
            )
            
    except Exception as e:
        error_message = f"Method recommendation error: {str(e)}"
        logger.error(error_message)
        return MethodRecommendationOutput(
            recommended_method=ProcessingMethod.PARSE_API_URL,
            explanation="An error occurred during recommendation. Defaulting to basic parser for safety."
        )

async def process_method_recommendation(complexity_option: str) -> Tuple[ProcessingMethod, str]:
    """Process method recommendation and return the method and explanation"""
    try:
        logger.info(f"Processing recommendation for complexity: {complexity_option}")
        
        # Create input parameters
        recommendation_input = MethodRecommendationInput(
            user_complexity_preference=complexity_option
        )
        
        # Run agent with proper deps parameter
        run_result = await method_recommendation_agent.run(
            recommendation_input.user_complexity_preference,
            deps=recommendation_input
        )
        
        if not run_result or not hasattr(run_result, 'data'):
            logger.warning("No valid response from recommendation agent, defaulting to PARSE_API_URL")
            return ProcessingMethod.PARSE_API_URL, "Defaulting to basic parser due to invalid response from recommendation agent."
        
        recommendation_output: MethodRecommendationOutput = run_result.data
        
        logger.info(f"Recommendation result: {recommendation_output.recommended_method}")
        return recommendation_output.recommended_method, recommendation_output.explanation
        
    except Exception as e:
        logger.error(f"Error getting method recommendation: {str(e)}")
        return ProcessingMethod.PARSE_API_URL, "Defaulting to basic parser due to error in recommendation."

# async def process_document(
#     file_data: bytes,
#     filename: str,
#     config: ProcessingConfig,
#     sem: asyncio.Semaphore,
#     update_file_status_func: Optional[callable] = None
# ) -> ProcessingResult:
#     """
#     Process a single document using the configured method.
    
#     Args:
#         file_data (bytes): The binary data of the file.
#         filename (str): The name of the file.
#         config (ProcessingConfig): Configuration for processing.
#         sem (asyncio.Semaphore): Semaphore to control concurrency.
#         update_file_status_func (Optional[callable]): Function to update file processing status.
    
#     Returns:
#         ProcessingResult: The result of processing the document.
#     """
#     logs = []

#     """Process document using configured method"""
#     try:
#         logs.append(f"Starting processing of {filename}")
#         if update_file_status_func:
#             update_file_status_func(filename, "ðŸ”„ Starting processing...", "running")

#         with NamedTemporaryFile(delete=False, suffix=Path(filename).suffix) as tmp:
#             tmp.write(file_data)
#             tmp_path = tmp.name
#         logs.append(f"Created temporary file {tmp_path}")
#         if update_file_status_func:
#             update_file_status_func(filename, "ðŸ“ Created temporary file", "running")

#         try:
#             if config.processing_method == ProcessingMethod.LLAMA_PARSER:
#                 if update_file_status_func:
#                     update_file_status_func(filename, "ðŸ” Parsing with LLAMA_PARSER...", "running")
#                 parser = LlamaParse(api_key=st.secrets.get("LLAMA_CLOUD_API_KEY"))
#                 json_data = parser.get_json_result(file_path=tmp_path)
#                 logs.append("LLAMA_PARSER returned JSON data.")
#                 if update_file_status_func:
#                     update_file_status_func(filename, "âœ… LLAMA_PARSER returned data", "running")

#                 documents = []
#                 for document_json in json_data:
#                     if "pages" not in document_json:
#                         logs.append("No 'pages' key found in parser result!")
#                         if update_file_status_func:
#                             update_file_status_func(filename, "âŒ No 'pages' in parser result", "error")
#                         raise ValueError("No 'pages' key in LLAMA_PARSER result.")
#                     for page in document_json["pages"]:
#                         doc = Document(
#                             text=page["text"],
#                             metadata={
#                                 "filename": filename,
#                                 "page_number": page["page"]
#                             }
#                         )
#                         documents.append(doc)

#                 logs.append(f"Extracted {len(documents)} documents.")
#                 if update_file_status_func:
#                     update_file_status_func(filename, f"âœ… Extracted {len(documents)} docs", "running")

#                 if 'document_store' not in st.session_state:
#                     st.session_state['document_store'] = []
#                 st.session_state['document_store'].extend(documents)
#                 logs.append("Stored documents in session.")
#                 if update_file_status_func:
#                     update_file_status_func(filename, "ðŸ’¾ Stored docs in session", "running")

#                 texts = [doc.text for doc in documents]

            
#             elif config.processing_method == ProcessingMethod.PARSE_API_URL:
#                 if update_file_status_func:
#                     update_file_status_func(filename, "ðŸ” Preparing to parse with API...", "running")
#                 from aiohttp import ClientTimeout
#                 import time

#                 async with aiohttp.ClientSession() as session:
#                     if update_file_status_func:
#                         update_file_status_func(filename, "ðŸ“¤ Preparing file upload...", "running")
                    
#                     form_data = aiohttp.FormData()
#                     form_data.add_field(
#                         'file',
#                         file_data,
#                         filename=filename,
#                         content_type='application/octet-stream'
#                     )

#                     timeout = ClientTimeout(total=300)  # matching the given example timeout
#                     start_time = time.time()
                    
#                     try:
#                         if update_file_status_func:
#                             update_file_status_func(filename, "ðŸš€ Sending request to API...", "running")
                        
#                         # Acquire semaphore before making the request
#                         async with sem, session.post(st.secrets['parse_api_url'], data=form_data, timeout=timeout) as response:
#                             if response.status != 200:
#                                 if update_file_status_func:
#                                     update_file_status_func(filename, f"âŒ API error: {response.status}", "error")
#                                 raise Exception(f"API error: {response.status}")
                            
#                             if update_file_status_func:
#                                 update_file_status_func(filename, "ðŸ“¥ Receiving response...", "running")
                            
#                             # Instead of directly using response.json(), mimic the logic from post_image_upload
#                             texts = await response.text()

#                     except Exception as e:
#                         logger.error(f"Error processing PARSE_API_URL: {str(e)}")
#                         if update_file_status_func:
#                             update_file_status_func(filename, f"âŒ Error: {str(e)}", "error")
#                         raise
#                     finally:
#                         end_time = time.time()
#                         elapsed_time = end_time - start_time
#                         logger.info(f"Execution time: {elapsed_time:.2f} seconds")
#                         if update_file_status_func:
#                             update_file_status_func(filename, f"â±ï¸ Request completed in {elapsed_time:.2f}s", "running")

#                 # Assuming the API returns plain text with each chunk separated by newlines
#                 texts = texts.split('\n') if texts else []
#                 logs.append(f"Extracted {len(texts)} text chunks.")
#                 if update_file_status_func:
#                     update_file_status_func(filename, f"âœ… Extracted {len(texts)} chunks in {elapsed_time:.2f}s", "running")
            
#             elif config.processing_method == ProcessingMethod.COLPALI:
#                 if update_file_status_func:
#                     update_file_status_func(filename, "ðŸ–¼ï¸ Processing with COLPALI...", "running")
                
#                 from transformers import pipeline
#                 vision_model = pipeline("document-question-answering", model="Salesforce/blip2-flan-t5-xl")
#                 result = vision_model(tmp_path, question="What is the content of this document?")
#                 texts = [result['answer']]

#                 logs.append("Extracted text from vision model.")
#                 if update_file_status_func:
#                     update_file_status_func(filename, "âœ… Extracted content from images", "running")

            
#             # Generate metadata for each text chunk
#             documents = []
#             for i, text in enumerate(texts):
#                 logs.append(f"Generating metadata for chunk {i}...")
#                 if update_file_status_func:
#                     update_file_status_func(filename, f"ðŸ§­ Metadata for chunk {i}", "running")
                
#                 metadata = await generate_document_metadata(text, filename, i, config)
#                 if not metadata:
#                     logger.error(f"Failed to generate metadata for {filename} chunk {i}")
#                     continue  # Skip this chunk
                
#                 doc_info = {
#                     "source_name": metadata['source_name'],
#                     "index": metadata['index'],
#                     "text_chunk": metadata['text_chunk'],
#                     "title": metadata['title'],
#                     "hashtags": metadata['hashtags'],
#                     "hypothetical_questions": metadata['hypothetical_questions'],
#                     "summary": metadata['summary'],
#                     "metadata": metadata['metadata']
#                 }
#                 logger.info(f"DocumentInfo: {doc_info}")

#                 documents.append(doc_info)

#             logs.append("All chunks processed successfully.")
#             if update_file_status_func:
#                 update_file_status_func(filename, "âœ… All chunks processed", "running")
            
#             return ProcessingResult(
#                 success=True,
#                 message="Document processed successfully",
#                 method_used=config.processing_method,
#                 document_info=DocumentInfo(
#                     source_name=metadata["source_name"],
#                     index=metadata["index"],
#                     text_chunk=metadata["text_chunk"],
#                     title=metadata["title"],
#                     hashtags=metadata["hashtags"],
#                     hypothetical_questions=metadata["hypothetical_questions"],
#                     summary=metadata["summary"],
#                     metadata=metadata.get("metadata", {})
#                 ),
#                 error=None                
#             )
            
#         except Exception as e:
#             error_msg = f"Error processing document {filename}: {str(e)}"
#             logger.error(error_msg)
#             logs.append(error_msg)
#             if update_file_status_func:
#                 update_file_status_func(filename, "âŒ Error occurred", "error")
#             return ProcessingResult(
#                 success=False,
#                 message="Error processing document",
#                 method_used=config.processing_method,
#                 error=str(e)
#             )
#         finally:
#             try:
#                 if 'tmp_path' in locals():
#                     os.unlink(tmp_path)
#                     logger.info(f"Deleted temporary file {tmp_path}")

#             except Exception as e:
#                 logger.warning(f"Could not delete temporary file {tmp_path}: {str(e)}")

#     except Exception as e:
#         error_msg = f"Error processing document {filename}: {str(e)}"
#         logger.error(error_msg)
#         logs.append(error_msg)
#         if update_file_status_func:
#             update_file_status_func(filename, "âŒ Error occurred", "error")
#         return ProcessingResult(
#             success=False,
#             message="Error processing document",
#             method_used=config.processing_method,
#             error=str(e)
#         )
#     finally:
#         logger.info(f"Processed document {filename} with logs: {logs}")
        
#         if update_file_status_func:
#             update_file_status_func(filename, "âœ… Document processed", "success")

#         return ProcessingResult(
#             success=True,
#             message="Document processed successfully",
#             method_used=config.processing_method,
#             error=None
#         )
        
async def process_document(
    file_data: bytes,
    filename: str,
    config: ProcessingConfig,
    sem: asyncio.Semaphore,
    update_file_status_func: Optional[callable] = None
) -> ProcessingResult:
    """
    Process a single document using the configured method.
    
    Args:
        file_data (bytes): The binary data of the file.
        filename (str): The name of the file.
        config (ProcessingConfig): Configuration for processing.
        sem (asyncio.Semaphore): Semaphore to control concurrency.
        update_file_status_func (Optional[callable]): Function to update file processing status.
    
    Returns:
        ProcessingResult: The result of processing the document.
    """
    logs = []
    tmp_path = None  # Initialize tmp_path
    
    try:
        logs.append(f"Starting processing of {filename}")
        if update_file_status_func:
            update_file_status_func(filename, "ðŸ”„ Starting processing...", "running")

        with NamedTemporaryFile(delete=False, suffix=Path(filename).suffix) as tmp:
            tmp.write(file_data)
            tmp_path = tmp.name
        logs.append(f"Created temporary file {tmp_path}")
        if update_file_status_func:
            update_file_status_func(filename, "ðŸ“ Created temporary file", "running")

        try:
            if config.processing_method == ProcessingMethod.LLAMA_PARSER:
                if update_file_status_func:
                    update_file_status_func(filename, "ðŸ” Parsing with LLAMA_PARSER...", "running")
                parser = LlamaParse(api_key=st.secrets.get("LLAMA_CLOUD_API_KEY"))
                json_data = parser.get_json_result(file_path=tmp_path)
                logs.append("LLAMA_PARSER returned JSON data.")
                if update_file_status_func:
                    update_file_status_func(filename, "âœ… LLAMA_PARSER returned data", "running")

                documents = []
                for document_json in json_data:
                    if "pages" not in document_json:
                        logs.append("No 'pages' key found in parser result!")
                        if update_file_status_func:
                            update_file_status_func(filename, "âŒ No 'pages' in parser result", "error")
                        raise ValueError("No 'pages' key in LLAMA_PARSER result.")
                    for page in document_json["pages"]:
                        doc = Document(
                            text=page["text"],
                            metadata={
                                "filename": filename,
                                "page_number": page["page"]
                            }
                        )
                        documents.append(doc)

                logs.append(f"Extracted {len(documents)} documents.")
                if update_file_status_func:
                    update_file_status_func(filename, f"âœ… Extracted {len(documents)} docs", "running")

                if 'document_store' not in st.session_state:
                    st.session_state['document_store'] = []
                st.session_state['document_store'].extend(documents)
                logs.append("Stored documents in session.")
                if update_file_status_func:
                    update_file_status_func(filename, "ðŸ’¾ Stored docs in session", "running")

                texts = [doc.text for doc in documents]

            elif config.processing_method == ProcessingMethod.PARSE_API_URL:
                if update_file_status_func:
                    update_file_status_func(filename, "ðŸ” Preparing to parse with API...", "running")
                from aiohttp import ClientTimeout
                import time

                async with aiohttp.ClientSession() as session:
                    if update_file_status_func:
                        update_file_status_func(filename, "ðŸ“¤ Preparing file upload...", "running")
                    
                    form_data = aiohttp.FormData()
                    form_data.add_field(
                        'file',
                        file_data,
                        filename=filename,
                        content_type='application/octet-stream'
                    )

                    timeout = ClientTimeout(total=300)  # matching the given example timeout
                    start_time = time.time()
                    
                    try:
                        if update_file_status_func:
                            update_file_status_func(filename, "ðŸš€ Sending request to API...", "running")
                        
                        # Acquire semaphore before making the request
                        async with sem, session.post(st.secrets['parse_api_url'], data=form_data, timeout=timeout) as response:
                            if response.status != 200:
                                if update_file_status_func:
                                    update_file_status_func(filename, f"âŒ API error: {response.status}", "error")
                                raise Exception(f"API error: {response.status}")
                            
                            if update_file_status_func:
                                update_file_status_func(filename, "ðŸ“¥ Receiving response...", "running")
                            
                            # Assuming the API returns plain text with each chunk separated by newlines
                            texts = await response.text()

                    except Exception as e:
                        logger.error(f"Error processing PARSE_API_URL: {str(e)}")
                        if update_file_status_func:
                            update_file_status_func(filename, f"âŒ Error: {str(e)}", "error")
                        raise
                    finally:
                        end_time = time.time()
                        elapsed_time = end_time - start_time
                        logger.info(f"Execution time: {elapsed_time:.2f} seconds")
                        if update_file_status_func:
                            update_file_status_func(filename, f"â±ï¸ Request completed in {elapsed_time:.2f}s", "running")

                # Split texts into chunks
                texts = texts.split('\n') if texts else []
                logs.append(f"Extracted {len(texts)} text chunks.")
                if update_file_status_func:
                    update_file_status_func(filename, f"âœ… Extracted {len(texts)} chunks in {elapsed_time:.2f}s", "running")
            
            elif config.processing_method == ProcessingMethod.COLPALI:
                if update_file_status_func:
                    update_file_status_func(filename, "ðŸ–¼ï¸ Processing with COLPALI...", "running")
                
                from transformers import pipeline
                vision_model = pipeline("document-question-answering", model="Salesforce/blip2-flan-t5-xl")
                result = vision_model(tmp_path, question="What is the content of this document?")
                texts = [result['answer']]

                logs.append("Extracted text from vision model.")
                if update_file_status_func:
                    update_file_status_func(filename, "âœ… Extracted content from images", "running")

            # Generate metadata for each text chunk
            documents = []
            for i, text in enumerate(texts):
                logs.append(f"Generating metadata for chunk {i}...")
                if update_file_status_func:
                    update_file_status_func(filename, f"ðŸ§­ Metadata for chunk {i}", "running")
                
                metadata = await generate_document_metadata(text, filename, i, config)
                if not metadata:
                    logger.error(f"Failed to generate metadata for {filename} chunk {i}")
                    continue  # Skip this chunk
                
                doc_info = {
                    "source_name": metadata['source_name'],
                    "index": metadata['index'],
                    "text_chunk": metadata['text_chunk'],
                    "title": metadata['title'],
                    "hashtags": metadata['hashtags'],
                    "hypothetical_questions": metadata['hypothetical_questions'],
                    "summary": metadata['summary'],
                    "metadata": metadata['metadata']
                }
                logger.info(f"DocumentInfo: {doc_info}")

                documents.append(doc_info)

            logs.append("All chunks processed successfully.")
            if update_file_status_func:
                update_file_status_func(filename, "âœ… All chunks processed", "running")
            
            # Create DocumentInfo instances and return ProcessingResult
            document_infos = []
            for doc in documents:
                document_info = DocumentInfo(**doc)
                document_infos.append(document_info)
            
            # For simplicity, let's assume you want to return the first DocumentInfo
            # Modify as needed to handle multiple DocumentInfo instances
            if document_infos:
                return ProcessingResult(
                    success=True,
                    message="Document processed successfully",
                    method_used=config.processing_method,
                    document_info=document_infos[0],  # Or handle multiple as needed
                    error=None
                )
            else:
                return ProcessingResult(
                    success=False,
                    message="No valid document info generated",
                    method_used=config.processing_method,
                    document_info=None,
                    error="No valid document info generated"
                )
                
        except Exception as e:
            error_msg = f"Error processing document {filename}: {str(e)}"
            logger.error(error_msg)
            logs.append(error_msg)
            if update_file_status_func:
                update_file_status_func(filename, "âŒ Error occurred", "error")
            return ProcessingResult(
                success=False,
                message="Error processing document",
                method_used=config.processing_method,
                document_info=None,
                error=str(e)
            )
        finally:
            try:
                if tmp_path and os.path.exists(tmp_path):
                    os.unlink(tmp_path)
                    logger.info(f"Deleted temporary file {tmp_path}")
            except Exception as e:
                logger.warning(f"Could not delete temporary file {tmp_path}: {str(e)}")

    except Exception as e:
        error_msg = f"Error processing document {filename}: {str(e)}"
        logger.error(error_msg)
        logs.append(error_msg)
        if update_file_status_func:
            update_file_status_func(filename, "âŒ Error occurred", "error")
        return ProcessingResult(
            success=False,
            message="Error processing document",
            method_used=config.processing_method,
            document_info=None,
            error=str(e)
        )

async def run_all_file_processing(pdf_files, image_files, excel_files, csv_files, other_files, option, llama_parser, colpali_processor, colpali_model):
    await asyncio.gather(
        process_pdf_file(pdf_files, option, llama_parser, colpali_processor, colpali_model),
        process_image_file(image_files, option, llama_parser, colpali_processor, colpali_model),
        process_excel_file(excel_files, option, llama_parser, colpali_processor, colpali_model),
        process_csv_file(csv_files, option, llama_parser, colpali_processor, colpali_model),
        process_other_file(other_files, option, llama_parser, colpali_processor, colpali_model),
    )

async def process_pdf_file(files, option, llama_parser, colpali_processor, colpali_model):
    config = ProcessingConfig(
        processing_method=ProcessingMethod.PARSE_API_URL if "parse_api_url" in option
        else ProcessingMethod.LLAMA_PARSER if "llama_parser" in option
        else ProcessingMethod.COLPALI if "colpali" in option
        else ProcessingMethod.PARSE_API_URL
    )
    
    for file in files:
        result = await process_document(
            file_data=file.getvalue(),
            filename=file.name,
            config=config,
            sem=sem
        )
        if result.success and result.document_info:
            add_unified_document_chunk(
                source_name=result.document_info['source_name'],
                index=result.document_info['index'],
                file_type="pdf",  # or "image" based on the file type
                text_chunk=result.document_info['text_chunk'],
                title=result.document_info['title'],
                hashtags=result.document_info['hashtags'],
                hypothetical_questions=result.document_info['hypothetical_questions'],
                summary=result.document_info['summary'],
                metadata=result.document_info.get('metadata', {})
            )
            logger.info(f"PDF file {file.name} processed successfully.")
        else:
            logger.error(f"Error processing PDF file {file.name}: {result.error}")

async def process_image_file(files, option, llama_parser, colpali_processor, colpali_model):
    config = ProcessingConfig(
        processing_method=ProcessingMethod.PARSE_API_URL if "parse_api_url" in option
        else ProcessingMethod.LLAMA_PARSER if "llama_parser" in option
        else ProcessingMethod.COLPALI if "colpali" in option
        else ProcessingMethod.PARSE_API_URL
    )
    
    for file in files:
        result = await process_document(
            file_data=file.getvalue(),
            filename=file.name,
            config=config,
            sem=sem
        )
        if result.success and result.document_info:
            add_unified_document_chunk(
                source_name=result.document_info['source_name'],
                index=result.document_info['index'],
                file_type="image",  # or "image" based on the file type
                text_chunk=result.document_info['text_chunk'],
                title=result.document_info['title'],
                hashtags=result.document_info['hashtags'],
                hypothetical_questions=result.document_info['hypothetical_questions'],
                summary=result.document_info['summary'],
                metadata=result.document_info.get('metadata', {})
            )

        else:
            logger.error(f"Error processing image file {file.name}: {result.error}")

async def process_excel_file(files, option, llama_parser, colpali_processor, colpali_model):
    config = ProcessingConfig(
        processing_method=ProcessingMethod.PARSE_API_URL if "parse_api_url" in option
        else ProcessingMethod.LLAMA_PARSER if "llama_parser" in option
        else ProcessingMethod.COLPALI if "colpali" in option
        else ProcessingMethod.PARSE_API_URL
    )
    
    for file in files:
        file_name = file.name
        df_file = pd.ExcelFile(file)
        for sheet in df_file.sheet_names:
            df = pd.read_excel(df_file, sheet_name=sheet)
            for row_index, row in df.iterrows():
                columns_values = ", ".join([f"{col}: {val}" for col, val in row.items()])
                text_chunk = f"{file_name} {sheet} row {row_index}: {columns_values}"
                
                metadata = await generate_document_metadata(text_chunk, file_name, row_index, config)
                if not metadata:
                    logger.error(f"Failed to generate metadata for {file_name} sheet {sheet} row {row_index}")
                    continue  # Skip this chunk
                
                add_unified_document_chunk(
                    source_name=metadata['source_name'],
                    index=metadata['index'],
                    file_type="excel",
                    text_chunk=metadata['text_chunk'],
                    title=metadata.get('title', ''),
                    hashtags=metadata.get('hashtags', []),
                    hypothetical_questions=metadata.get('hypothetical_questions', []),
                    summary=metadata.get('summary', ''),
                    metadata={
                        "sheet_name": sheet,
                        "row_number": row_index,
                        "original_values": row.to_dict()
                    }
                )

async def process_csv_file(files, option, llama_parser, colpali_processor, colpali_model):
    config = ProcessingConfig(
        processing_method=ProcessingMethod.PARSE_API_URL if "parse_api_url" in option
        else ProcessingMethod.LLAMA_PARSER if "llama_parser" in option
        else ProcessingMethod.COLPALI if "colpali" in option
        else ProcessingMethod.PARSE_API_URL
    )
    
    for file in files:
        file_name = file.name
        df = pd.read_csv(file)
        for row_index, row in df.iterrows():
            columns_values = ", ".join([f"{col}: {val}" for col, val in row.items()])
            text_chunk = f"{file_name} Row {row_index}: {columns_values}"
            
            metadata = await generate_document_metadata(text_chunk, file_name, row_index, config)
            if not metadata:
                logger.error(f"Failed to generate metadata for {file_name} row {row_index}")
                continue  # Skip this chunk
            
            add_unified_document_chunk(
                source_name=metadata['source_name'],
                index=metadata['index'],
                file_type="csv",
                text_chunk=metadata['text_chunk'],
                title=metadata.get('title', ''),
                hashtags=metadata.get('hashtags', []),
                hypothetical_questions=metadata.get('hypothetical_questions', []),
                summary=metadata.get('summary', ''),
                metadata={
                    "row_number": row_index,
                    "original_values": row.to_dict()
                }
            )


async def process_other_file(files, option, llama_parser, colpali_processor, colpali_model):
    config = ProcessingConfig(
        processing_method=ProcessingMethod.PARSE_API_URL if "parse_api_url" in option
        else ProcessingMethod.LLAMA_PARSER if "llama_parser" in option
        else ProcessingMethod.COLPALI if "colpali" in option
        else ProcessingMethod.PARSE_API_URL
    )
    
    for file in files:
        result = await process_document(
            file_data=file.getvalue(),
            filename=file.name,
            config=config,
            sem=sem
        )
        if result.success and result.document_info:
            add_unified_document_chunk(
                source_name=result.document_info['source_name'],
                index=result.document_info['index'],
                file_type="other",  # or "image" based on the file type
                text_chunk=result.document_info['text_chunk'],
                title=result.document_info['title'],
                hashtags=result.document_info['hashtags'],
                hypothetical_questions=result.document_info['hypothetical_questions'],
                summary=result.document_info['summary'],
                metadata=result.document_info.get('metadata', {})
            )

        else:
            logger.error(f"Error processing other file {file.name}: {result.error}")

################################################################################################
### Agent for document info used to parse and pre-process text extracted from files ###
################################################################################################
import asyncio
import logging
from typing import List, Optional, Any, Dict
from pydantic import BaseModel, Field
from pydantic_ai import Agent
import streamlit as st
import hashlib

logger = logging.getLogger(__name__)

class AgentMetadata(BaseModel):
    """Metadata generated by the agent."""
    title: str
    hashtags: List[str] = Field(description="List of hashtags for the document")
    hypothetical_questions: List[str]
    summary: str

# Initialize the Agent with the desired result type and system prompt
rate_limited_query_message_info_async_agent = Agent(
    model="openai:gpt-4o-mini",
    result_type=AgentMetadata,
    system_prompt=(
        "You are a helpful assistant. Given a piece of text (a document chunk), "
        "you must output a JSON structure with the following fields:\n"
        "1. title: str\n"
        "2. hashtags: List[str]\n"
        "3. hypothetical_questions: List[str]\n"
        "4. summary: str\n\n"
        "Follow these instructions strictly and return only the JSON data."
    )
)

async def rate_limited_query_message_info_async(
    message_data: str,
    sem: asyncio.Semaphore,
    filename: str,
    index: int
) -> Dict[str, Any]:
    """
    Rate-limited query to the Agent for generating document metadata.
    
    Args:
        message_data (str): The text chunk from the document.
        sem (asyncio.Semaphore): Semaphore to control concurrency.
        filename (str): Name of the source file.
        index (int): Index of the chunk within the document.
    
    Returns:
        dict: A dictionary containing the combined metadata.
    """
    async with sem:
        try:
            # Create a cache key based on the message_data
            cache_key = f"metadata_cache_{hashlib.sha256(message_data.encode()).hexdigest()}"
            
            # Initialize metadata_cache in session state if not present
            if 'metadata_cache' not in st.session_state:
                st.session_state.metadata_cache = {}
            
            # Return cached result if available
            if cache_key in st.session_state.metadata_cache:
                logger.info("Using cached metadata")
                return st.session_state.metadata_cache[cache_key]
            
            # Use the Agent to process the message_data
            run_result = await rate_limited_query_message_info_async_agent.run(message_data)
            
            # Extract the AgentMetadata from RunResult via .data
            if not hasattr(run_result, 'data'):
                raise AttributeError("RunResult object has no attribute 'data'")
            
            agent_metadata: AgentMetadata = run_result.data  # Correct attribute access
            
            # Convert the Pydantic model to dict
            metadata = agent_metadata.model_dump()
            
            # Combine with external fields
            combined_metadata = {
                "source_name": filename,
                "index": index,
                "text_chunk": message_data.strip().replace('\n\n', '. '),
                "title": metadata["title"],
                "hashtags": metadata["hashtags"],
                "hypothetical_questions": metadata["hypothetical_questions"],
                "summary": metadata["summary"],
                "metadata": {}  # Additional metadata can be added here
            }
            
            # Cache the results
            st.session_state.metadata_cache[cache_key] = combined_metadata
            logger.info(f"Metadata cached for key: {cache_key}")
            return combined_metadata

        except Exception as e:
            logger.error(f"Error in rate_limited_query_message_info_async: {str(e)}")
            # Attempt to find a similar cached result
            for cached_key, cached_value in st.session_state.get('metadata_cache', {}).items():
                if cached_value.get('text_chunk', '').strip() == message_data.strip():
                    logger.info("Using similar cached metadata as fallback")
                    return cached_value
            # If no cache is found, re-raise the exception
            raise

async def generate_document_metadata(
    content: str,
    filename: str,
    index: int,
    config: Optional[Any] = None
) -> Dict[str, Any]:
    """
    Generate metadata for a document chunk using Agent-based parsing.
    
    Args:
        content (str): The text content of the document chunk.
        filename (str): The name of the source file.
        index (int): The index of the chunk within the document.
        config (Optional[Any]): Processing configuration.
    
    Returns:
        dict: A dictionary containing the complete metadata.
    """
    try:
        # Create a unique cache key for this document chunk
        cache_key = f"doc_metadata_{filename}_{index}"
        
        # Initialize doc_metadata_cache in session state if not present
        if 'doc_metadata_cache' not in st.session_state:
            st.session_state.doc_metadata_cache = {}
        
        # Return cached metadata if available
        if cache_key in st.session_state.doc_metadata_cache:
            logger.info(f"Using cached document metadata for {filename} chunk {index}")
            return st.session_state.doc_metadata_cache[cache_key]
        
        # Prepare the message_data for the Agent
        message_data = f"Generate metadata for the following document chunk:\n\n{content[:1500]}..."
        
        # Initialize semaphore in Streamlit's session state if not already done
        if 'sem' not in st.session_state:
            max_concurrent_requests = 10  # Adjust based on your rate limits
            st.session_state.sem = asyncio.Semaphore(max_concurrent_requests)
        
        sem = st.session_state.sem
        
        # Call the rate-limited Agent function
        metadata = await rate_limited_query_message_info_async(message_data, sem, filename, index)
        
        # Validate required fields
        required_fields = ["source_name", "index", "text_chunk", "title", "hashtags", "hypothetical_questions", "summary"]
        if not all(field in metadata for field in required_fields):
            logger.error(f"Missing fields in metadata: {metadata}")
            # Attempt to retrieve from doc_metadata_cache based on filename and index
            for cached_key, cached_value in st.session_state.doc_metadata_cache.items():
                if cached_key.startswith(f"doc_metadata_{filename}_") and cached_value.get('index') == index:
                    logger.info(f"Using cached metadata for {filename} chunk {index}")
                    return cached_value
            # If no cache is found, return an empty dict
            return {}
        
        # Ensure hashtags is a list of strings
        hashtags = metadata["hashtags"]
        if isinstance(hashtags, str):
            hashtags = hashtags.split()  # Split by spaces if it's a single string
        elif not isinstance(hashtags, list):
            hashtags = [str(hashtags)]
        
        # Similarly, ensure hypothetical_questions is a list of strings
        hypothetical_questions = metadata.get("hypothetical_questions", [])
        if isinstance(hypothetical_questions, str):
            hypothetical_questions = [hypothetical_questions]
        elif not isinstance(hypothetical_questions, list):
            hypothetical_questions = [str(hypothetical_questions)]
        
        # Create document info dictionary
        doc_info = DocumentInfo(
            source_name=metadata["source_name"],
            index=metadata["index"],
            text_chunk=metadata["text_chunk"],
            title=metadata["title"],
            hashtags=hashtags,
            hypothetical_questions=hypothetical_questions,
            summary=metadata["summary"],
            metadata=metadata.get("metadata", {})
        )
        logger.info(f"DocumentInfo: {doc_info}")

        
        # Cache the document info
        st.session_state.doc_metadata_cache[cache_key] = doc_info.model_dump()
        logger.info(f"Metadata cached for {filename} chunk {index}")
        return doc_info.model_dump()

    except Exception as e:
        logger.error(f"Error generating metadata: {str(e)}")
        # Attempt to retrieve from doc_metadata_cache if available
        for cached_key, cached_value in st.session_state.get('doc_metadata_cache', {}).items():
            if cached_key.startswith(f"doc_metadata_{filename}_"):
                logger.info(f"Using cached metadata for {filename} chunk {index} due to error")
                return cached_value
        # If no cache is found, return an empty dict
        return {}


################################################################################################
### Function for generating unified document chunks ###
################################################################################################


def retry_async(retries=3, delay=1):
    """Retry decorator for async functions"""
    def decorator(func):
        async def wrapper(*args, **kwargs):
            for attempt in range(retries):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    if attempt == retries - 1:
                        logger.error(f"Failed after {retries} attempts: {str(e)}")
                        raise
                    logger.warning(f"Attempt {attempt + 1} failed: {str(e)}")
                    await asyncio.sleep(delay * (attempt + 1))
            return None
        return wrapper
    return decorator

@retry_async(retries=3)
async def setup_hybrid_collection(documents_referred: str, config: ProcessingConfig) -> VectorStoreIndex:
    """Set up hybrid collection with vector store"""
    try:
        collection_name = f"{documents_referred}_documents_referred".replace('\\', '_').replace(':', '_')
        
        # Initialize Qdrant clients
        qdrant_client = QdrantClient(
            url=st.secrets["qdrant_url"],
            api_key=st.secrets["qdrant_api_key"]
        )
        qdrant_aclient = AsyncQdrantClient(
            url=st.secrets["qdrant_url"],
            api_key=st.secrets["qdrant_api_key"]
        )
        
        # Create collection if it doesn't exist
        if not qdrant_client.collection_exists(collection_name=collection_name):
            # Create a new collection with hybrid search configuration
            qdrant_client.create_collection(
                collection_name=collection_name,
                vectors_config={
                    "text-dense": models.VectorParams(
                        size=1536,  # OpenAI Embeddings
                        distance=models.Distance.COSINE,
                    )
                },
                sparse_vectors_config={
                    "text-sparse": models.SparseVectorParams(
                        index=models.SparseIndexParams(
                            on_disk=False,
                        )
                    )
                },
            )

        # Initialize vector store with hybrid search
        vector_store = QdrantVectorStore(
            collection_name=collection_name,
            client=qdrant_client,
            aclient=qdrant_aclient,
            enable_hybrid=True,
            batch_size=20,
            fastembed_sparse_model="Qdrant/bm42-all-minilm-l6-v2-attentions",
        )

        # Initialize storage context and set chunk size
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        Settings.chunk_size = 512

        # Set up embedding model based on config
        if config.azure_openai_key:
            embed_model = AzureOpenAIEmbedding(
                model=config.embedding_model,
                deployment_name=config.embedding_model,
                api_key=config.azure_openai_key,
                azure_endpoint=config.azure_endpoint
            )
        else:
            embed_model = OpenAIEmbedding(
                model=config.embedding_model,
                api_key=config.openai_key
            )

        # Create index from documents
        if 'document_store' not in st.session_state:
            logger.error("No documents found in session state for indexing")
            return None

        # Initialize the hybrid index
        index = VectorStoreIndex.from_documents(
            documents=st.session_state['document_store'],
            embed_model=embed_model,
            storage_context=storage_context,
        )

        return index
        
    except Exception as e:
        logger.error(f"Error setting up hybrid collection: {str(e)}")
        raise


@retry_async(retries=3)
async def hybrid_search(
    query: str,
    documents_referred: str,
    index: VectorStoreIndex,
    limit: int = 5
) -> List[dict]:
    """Perform hybrid search using the vector store index with both dense and sparse vectors"""
    try:
        if not index:
            raise ValueError("No valid index provided for search")

        # Initialize query engine with hybrid search configuration
        query_engine = index.as_query_engine(
            similarity_top_k=2,     # Dense vector top k
            sparse_top_k=12,        # Sparse vector top k
            vector_store_query_mode="hybrid"
        )
        
        try:
            # Execute the query
            response = await query_engine.aquery(query)
            
            # Convert results to DocumentInfo objects
            documents = []
            for node in response.source_nodes:
                doc_info = {
                    "source_name": node.metadata.get("source", ""),
                    "index": node.metadata.get("index", 0),
                    "title": node.metadata.get("title", ""),
                    "hashtags": node.metadata.get("hashtags", []),
                    "hypothetical_questions": node.metadata.get("hypothetical_questions", []),
                    "summary": node.metadata.get("summary", ""),
                    "text_chunk": node.text,
                    "metadata": {
                        "score": node.score,
                        "doc_id": node.node_id,
                        **node.metadata
                    }
                }
                logger.info(f"DocumentInfo: {doc_info}")

                documents.append(doc_info)
            
            return documents[:limit]
            
        except Exception as e:
            logger.error(f"Error executing query: {str(e)}")
            raise
            
    except Exception as e:
        logger.error(f"Error in hybrid search: {str(e)}")
        raise

def add_unified_document_chunk(
    source_name: str,
    index: int,
    file_type: str,
    text_chunk: str,
    title: str,
    hashtags: List[str],
    hypothetical_questions: List[str],
    summary: str,
    metadata: Dict[str, Any]
):
    """
    Add a document chunk to unified storage with vector store integration.
    
    Args:
        source_name (str): Name or URL of the source document.
        index (int): Index of the chunk within the document.
        file_type (str): Type of the file (e.g., pdf, image).
        text_chunk (str): The text content of the chunk.
        title (str): Title of the chunk.
        hashtags (List[str]): Hashtags associated with the chunk.
        hypothetical_questions (List[str]): List of hypothetical questions related to the chunk.
        summary (str): Summary of the chunk.
        metadata (Dict[str, Any]): Additional metadata.
    """
    try:
        unique_key = f"{source_name}_{index}"
        record = {
            "source_name": source_name,
            "index": index,
            "title": title,
            "hashtags": hashtags,
            "hypothetical_questions": hypothetical_questions,
            "summary": summary,
            "text_chunk": text_chunk,
            "metadata": {
                "file_type": file_type,
                **metadata
            }
        }
        
        if 'unified_documents' not in st.session_state:
            st.session_state['unified_documents'] = {}
        
        st.session_state['unified_documents'][unique_key] = record
        
        # Create Document object for vector store indexing
        doc = Document(
            text=text_chunk,
            metadata={
                "source": source_name,
                "index": index,
                "title": title,
                "hashtags": hashtags,
                "hypothetical_questions": hypothetical_questions,
                "summary": summary,
                "file_type": file_type,
                **metadata
            }
        )
        
        if 'document_store' not in st.session_state:
            st.session_state['document_store'] = []
        
        st.session_state['document_store'].append(doc)
        logger.info(f"Added document chunk to unified storage: {unique_key}")
        
    except Exception as e:
        logger.error(f"Error adding document chunk: {str(e)}")
        raise



# #########################################################
# # Streamlit UI
# #########################################################

# def get_method_display_name(method: ProcessingMethod) -> str:
#     """Convert ProcessingMethod enum to display name"""
#     method_names = {
#         ProcessingMethod.PARSE_API_URL: "Basic Parser",
#         ProcessingMethod.LLAMA_PARSER: "LLAMA Parser",
#         ProcessingMethod.COLPALI: "COLPALI Vision Parser"
#     }
#     return method_names.get(method, "Unknown Method")

# # Define a mapping for predefined complexity options
# PREDEFINED_RECOMMENDATIONS = {
#     "Simple text-based document": {
#         "method": ProcessingMethod.PARSE_API_URL,
#         "explanation": "For simple text-based documents, the basic parsing API is fast and efficient for straightforward content extraction."
#     },
#     "Complex document (no images/diagrams)": {
#         "method": ProcessingMethod.LLAMA_PARSER,
#         "explanation": "For complex documents without images, the LLAMA parser excels at handling intricate structures and relationships."
#     },
#     "Complex document with images/diagrams": {
#         "method": ProcessingMethod.COLPALI,
#         "explanation": "For documents containing images or diagrams, COLPALI provides advanced vision capabilities for comprehensive analysis."
#     }
# }

# def get_predefined_method_recommendation(option: str) -> Tuple[ProcessingMethod, str]:
#     """Retrieve the processing method and explanation based on predefined options."""
#     recommendation = PREDEFINED_RECOMMENDATIONS.get(option)
#     if recommendation:
#         return recommendation["method"], recommendation["explanation"]
#     else:
#         # Default fallback
#         return ProcessingMethod.PARSE_API_URL, "Defaulting to basic parser for general document processing."

# async def display_file_upload_ui():
#     """Display the file upload UI and handle document processing"""
#     st.title("ðŸ“„ Document Processing System")
    
#     # Initialize semaphore in session state if not already present
#     if 'sem' not in st.session_state:
#         max_concurrent_requests = 10  # Adjust based on your rate limits
#         st.session_state.sem = asyncio.Semaphore(max_concurrent_requests)
    
#     # File uploader
#     uploaded_files = st.file_uploader(
#         "ðŸ“‚ Upload documents",
#         accept_multiple_files=True,
#         type=[
#             "pdf", "docx", "doc", "odt", "pptx", "ppt", "xlsx", "csv",
#             "tsv", "eml", "msg", "rtf", "epub", "html", "xml",
#             "png", "jpg", "jpeg", "txt"
#         ],
#         help="Select multiple files to upload for processing."
#     )
    
#     if uploaded_files:
#         st.write("### ðŸ“ Document Complexity")
        
#         if 'input_method' not in st.session_state:
#             st.session_state.input_method = None
#         if 'method' not in st.session_state:
#             st.session_state.method = None
            
#         # Choose input method: Text or Dropdown
#         input_method = st.radio(
#             "ðŸ” Choose how to specify document complexity:",
#             ("Provide a textual description", "Select from predefined options"),
#             horizontal=True
#         )
        
#         st.session_state.input_method = input_method
        
#         method = None
#         explanation = None
        
#         if st.session_state.input_method == "Provide a textual description":
#             complexity_description = st.text_area(
#                 "âœï¸ Describe the complexity and characteristics of your document:",
#                 help="Provide details such as the presence of images, diagrams, the structure's intricacy, etc."
#             )
            
#             submit = st.button("âœ… Submit", type="primary")
            
#             if submit:
#                 # Check if the description is sufficiently detailed
#                 if len(complexity_description.strip()) > 5:
#                     # Get processing method recommendation using the agent
#                     with st.spinner("ðŸ”„ Analyzing your description to recommend a processing method..."):
#                         method, explanation = await process_method_recommendation(complexity_description)
#                 else:
#                     st.warning("âš ï¸ Please provide a more detailed description (at least 5 characters).")
#                     st.stop()
        
#         else:
#             # Dropdown for predefined options
#             complexity_option = st.selectbox(
#                 "ðŸ“Š Select document complexity:",
#                 list(PREDEFINED_RECOMMENDATIONS.keys()),
#                 help="This helps us choose the best processing method for your documents."
#             )
            
#             submit = st.button("âœ… Submit", type="primary")
            
#             if submit:
#                 # Get processing method recommendation based on the selected option
#                 with st.spinner("ðŸ”„ Determining the best processing method based on your selection..."):
#                     method, explanation = get_predefined_method_recommendation(complexity_option)
        
#         st.session_state.method = method
        
#         # After submission, display the recommendation if available
#         if (st.session_state.input_method == "Provide a textual description" and st.session_state.method is not None) or \
#            (st.session_state.input_method == "Select from predefined options" and st.session_state.method is not None):
            
#             method_name = get_method_display_name(st.session_state.method)
#             st.info(f"**âœ… Recommended Processing Method:** {method_name}")
#             st.write(f"**ðŸ“ Reason:** {explanation}")
            
#             # Add confirmation step
#             st.write("### ðŸ”„ Confirm Processing")
#             st.write(f"**ðŸ“„ Number of files to process:** {len(uploaded_files)}")
#             for file in uploaded_files:
#                 st.write(f"- {file.name}")
            
#             col1, col2 = st.columns(2)
#             with col1:
#                 confirm = st.button("âœ… Confirm and Process Files", type="primary")
#             with col2:
#                 cancel = st.button("âŒ Cancel", type="secondary")
            
#             if confirm:
#                 status_container = st.empty()
#                 try:
#                     # Initialize config
#                     config = ProcessingConfig(
#                         azure_openai_key=st.secrets.get("AZURE_OPENAI_API_KEY"),
#                         azure_endpoint=st.secrets.get("AZURE_OPENAI_ENDPOINT"),
#                         openai_key=st.secrets.get("OPENAI_API_KEY"),
#                         processing_method=st.session_state.method
#                     )

#                     # Create a dictionary to hold individual file statuses
#                     file_status_mapping = {}
#                     for file in uploaded_files:
#                         file_status_mapping[file.name] = st.empty()
#                         file_status_mapping[file.name].text(f"â³ Waiting to process {file.name}...")
                    
#                     # Define a callback that process_document can use to update status
#                     def update_file_status(filename: str, message: str, state: str = "running"):
#                         """
#                         Update the processing status for a specific file.
                        
#                         Args:
#                             filename (str): Name of the file being processed.
#                             message (str): Status message to display.
#                             state (str): State of the processing ('running', 'error', 'complete').
#                         """
#                         if filename in file_status_mapping:
#                             # Prepend emojis based on state
#                             emoji = "ðŸ”„" if state == "running" else "âŒ" if state == "error" else "âœ…"
#                             file_status_mapping[filename].markdown(f"{emoji} {message}")
    
#                     # Create tasks for processing each file
#                     tasks = []
#                     for file in uploaded_files:
#                         file_content = file.read()
#                         tasks.append(process_document(
#                             file_data=file_content,
#                             filename=file.name,
#                             config=config,
#                             sem=st.session_state.sem,
#                             update_file_status_func=update_file_status
#                         ))
                    
#                     # Process files concurrently
#                     with st.spinner("ðŸ”„ Processing files..."):
#                         processed_results = await asyncio.gather(*tasks, return_exceptions=True)
                    
#                     # Handle results
#                     success_count = 0
#                     for file, result in zip(uploaded_files, processed_results):
#                         if isinstance(result, Exception):
#                             logger.error(f"Error processing {file.name}: {str(result)}")
#                             update_file_status(file.name, f"âŒ Error: {str(result)}", state="error")
#                         elif result.success and result.document_info:
#                             add_unified_document_chunk(
#                                 source_name=result.document_info.source_name,
#                                 index=result.document_info.index,
#                                 file_type=Path(file.name).suffix.lower().strip('.'),
#                                 text_chunk=result.document_info.text_chunk,
#                                 title=result.document_info.title,
#                                 hashtags=result.document_info.hashtags,
#                                 hypothetical_questions=result.document_info.hypothetical_questions,
#                                 summary=result.document_info.summary,
#                                 metadata=result.document_info.metadata
#                             )
#                             update_file_status(file.name, "âœ… Processing complete!", state="complete")
#                             success_count += 1
#                         else:
#                             logger.error(f"Error processing {file.name}: {result.message}")
#                             update_file_status(file.name, f"âŒ Error: {result.message}", state="error")
                    
#                     # After processing all files, set up the search index
#                     if success_count > 0:
#                         with st.spinner("ðŸ” Setting up search index..."):
#                             try:
#                                 index = await setup_hybrid_collection("hybrid_search", config)
#                                 if index:
#                                     st.session_state['search_index'] = index
#                                     st.success(f"ðŸŽ‰ Successfully processed {success_count}/{len(uploaded_files)} files.")
                                    
#                                     # Enable search
#                                     st.write("### ðŸ” Search Documents")
#                                     query = st.text_input("ðŸ–‹ï¸ Enter your search query:")
#                                     if query:
#                                         with st.spinner("ðŸ”„ Executing search..."):
#                                             results = await hybrid_search(query, "hybrid_search", index)
#                                             if results:
#                                                 for doc in results:
#                                                     st.write("---")
#                                                     st.write(f"**ðŸ“„ Title:** {doc['title']}")
#                                                     st.write(f"**ðŸ“ Summary:** {doc['summary']}")
#                                                     st.write(f"**ðŸ“ Source:** {doc['source_name']}")
#                                                     st.write(f"**ðŸ·ï¸ Tags:** {', '.join(doc['hashtags'])}")
#                                                     st.write(f"**ðŸ—’ï¸ Text Chunk:** {doc['text_chunk']}")
#                                             else:
#                                                 st.info("â„¹ï¸ No relevant documents found.")
#                                 else:
#                                     st.error("âš ï¸ Failed to create search index.")
#                             except Exception as e:
#                                 error_msg = f"âŒ Error setting up search index: {str(e)}"
#                                 logger.error(error_msg)
#                                 st.error(error_msg)
#                     else:
#                         st.error("âš ï¸ No files were successfully processed.")
                
#                 except Exception as e:
#                     error_msg = f"âŒ Error during document processing: {str(e)}"
#                     logger.error(error_msg)
#                     st.error(error_msg)
            
#             elif cancel:
#                 st.warning("ðŸš« File processing canceled.")
#                 st.stop()



# if __name__ == "__main__":

#     tests_passed = {}
    
#     async def test_generate_document_metadata():
#         content = "This is a sample document chunk. It contains important information about the topic."
#         filename = "sample_document.pdf"
#         index = 1
#         config = ProcessingConfig(
#             openai_key=st.secrets['OPENAI_API_KEY'],
#             processing_method=ProcessingMethod.PARSE_API_URL
#         )

#         metadata = await generate_document_metadata(content, filename, index, config)
#         print(metadata)
        
#         if isinstance(metadata, Exception):
#             tests_passed["generate_document_metadata"] = False
#         else:
#             tests_passed["generate_document_metadata"] = True

#     # Run the test
#     asyncio.run(test_generate_document_metadata())
    
#     # Expected output:
#     # {
#     #     "source_name": "sample_document.pdf",
#     #     "index": 1,
#     #     "text_chunk": "This is a sample document chunk. It contains important information about the topic.",
#     #     "title": "Sample Title",
#     #     "hashtags": ["#example", "#sample"],
#     #     "hypothetical_questions": ["What is the main topic?", "How does this relate to...?"],
#     #     "summary": "This is a summary of the document chunk.",
#     #     "metadata": {}
#     # }
    
    
#     async def test_generate_document_metadata_error():
#         content = ""  # Empty content to simulate an error
#         filename = "empty_document.pdf"
#         index = 2
#         config = ProcessingConfig(
#             openai_key="your_openai_key",
#             processing_method=ProcessingMethod.PARSE_API_URL
#         )

#         metadata = await generate_document_metadata(content, filename, index, config)
#         print(metadata)  # Should be {} or attempt to retrieve from cache
        
#         if isinstance(metadata, Exception):
#             tests_passed["generate_document_metadata_error"] = False
#         else:
#             tests_passed["generate_document_metadata_error"] = True

#     # Run the test
#     asyncio.run(test_generate_document_metadata_error())

#     # Expected output:
#     # {}
    
#     import asyncio

#     async def test_concurrent_metadata_generation():
#         tasks = []
#         for i in range(10):  # More than semaphore limit to test concurrency
#             content = f"Sample content for document chunk {i}."
#             filename = f"document_{i}.pdf"
#             index = i
#             tasks.append(generate_document_metadata(content, filename, index, None))
        
#         results = await asyncio.gather(*tasks, return_exceptions=True)
#         for result in results:
#             print(result)
        
#         if any(isinstance(r, Exception) for r in results):
#             tests_passed["concurrent_metadata_generation"] = False
#         else:
#             tests_passed["concurrent_metadata_generation"] = True

#     # Run the test
#     asyncio.run(test_concurrent_metadata_generation())

#     # Expected output: 
#     # A list of dictionaries containing metadata for each chunk, utilizing caching where applicable.

#     async def inspect_run_result():
#         message_data = "Sample document chunk for testing."
#         filename = "test_document.pdf"
#         index = 0
#         sem = asyncio.Semaphore(10)
        
#         try:
#             run_result = await rate_limited_query_message_info_async(message_data, sem, filename, index)
#             logger.info(f"RunResult type: {type(run_result)}")
#             logger.info(f"RunResult attributes: {dir(run_result)}")
#             tests_passed["inspect_run_result"] = True
#         except Exception as e:
#             logger.error(f"Error inspecting run_result: {str(e)}")
#             tests_passed["inspect_run_result"] = False

#     # Run the inspection
#     asyncio.run(inspect_run_result())


#     # Expected output:
#     # A RunResult object containing the result of the agent's run.
    
#     # Print whether the tests passed or failed
#     print("\nTest Results:")
    
#     if all(tests_passed.values()):
#         for test, passed in tests_passed.items():
#             print(f"Test {test}: {'Passed' if passed else 'Failed'}")
#         print("All tests passed!")
#     else:
#         for test, passed in tests_passed.items():
#             print(f"Test {test}: {'Passed' if passed else 'Failed'}")
#         print("Some tests failed.")
    
#     print("\n")
    
#     ########################################
#     # Initialize Streamlit UI
#     asyncio.run(display_file_upload_ui())

#     ########################################

#     # Update Dependencies:

#     # Ensure all packages, especially pydantic_ai, are up-to-date to benefit from the latest features and bug fixes.
#     # bash
#     # Copy code
#     # pip install --upgrade pydantic_ai
#     # Consult Documentation:

#     # Refer to the pydantic_ai documentation for detailed information on the Agent class and the structure of RunResult.
#     # Asynchronous Context in Streamlit:

#     # Streamlit's support for asynchronous functions is evolving. Depending on your Streamlit version, you might need to use st.experimental_async or other experimental features. Always refer to the latest Streamlit documentation for guidance.

#########################################################
# Streamlit UI
#########################################################

def get_method_display_name(method: ProcessingMethod) -> str:
    """Convert ProcessingMethod enum to display name"""
    method_names = {
        ProcessingMethod.PARSE_API_URL: "Basic Parser",
        ProcessingMethod.LLAMA_PARSER: "LLAMA Parser",
        ProcessingMethod.COLPALI: "COLPALI Vision Parser"
    }
    return method_names.get(method, "Unknown Method")

# Define a mapping for predefined complexity options
PREDEFINED_RECOMMENDATIONS = {
    "Simple text-based document": {
        "method": ProcessingMethod.PARSE_API_URL,
        "explanation": "For simple text-based documents, the basic parsing API is fast and efficient for straightforward content extraction."
    },
    "Complex document (no images/diagrams)": {
        "method": ProcessingMethod.LLAMA_PARSER,
        "explanation": "For complex documents without images, the LLAMA parser excels at handling intricate structures and relationships."
    },
    "Complex document with images/diagrams": {
        "method": ProcessingMethod.COLPALI,
        "explanation": "For documents containing images or diagrams, COLPALI provides advanced vision capabilities for comprehensive analysis."
    }
}

def get_predefined_method_recommendation(option: str) -> Tuple[ProcessingMethod, str]:
    """Retrieve the processing method and explanation based on predefined options."""
    recommendation = PREDEFINED_RECOMMENDATIONS.get(option)
    if recommendation:
        return recommendation["method"], recommendation["explanation"]
    else:
        # Default fallback
        return ProcessingMethod.PARSE_API_URL, "Defaulting to basic parser for general document processing."

def update_file_status_wrapper(filename: str, message: str, state: str = "running", file_status_mapping: Dict[str, Any] = {}):
    """
    Wrapper function to update file processing status.
    
    Args:
        filename (str): Name of the file being processed.
        message (str): Status message to display.
        state (str): State of the processing ('running', 'error', 'complete').
        file_status_mapping (dict): Mapping of file names to Streamlit placeholders.
    """
    if filename in file_status_mapping:
        # Prepend emojis based on state
        emoji = "ðŸ”„" if state == "running" else "âŒ" if state == "error" else "âœ…"
        file_status_mapping[filename].markdown(f"{emoji} {message}")

def display_file_upload_ui():
    """Display the file upload UI and handle document processing"""
    st.title("ðŸ“„ Document Processing System")
    
    # Initialize semaphore in session state if not already present
    if 'sem' not in st.session_state:
        max_concurrent_requests = 10  # Adjust based on your rate limits
        st.session_state.sem = asyncio.Semaphore(max_concurrent_requests)
    
    # File uploader
    uploaded_files = st.file_uploader(
        "ðŸ“‚ Upload documents",
        accept_multiple_files=True,
        type=[
            "pdf", "docx", "doc", "odt", "pptx", "ppt", "xlsx", "csv",
            "tsv", "eml", "msg", "rtf", "epub", "html", "xml",
            "png", "jpg", "jpeg", "txt"
        ],
        help="Select multiple files to upload for processing."
    )
    
    if uploaded_files:
        st.markdown("### ðŸ“ Document Complexity")
        
        # Input Method Selection
        input_method = st.selectbox(
            "ðŸ” Choose input method for document complexity:",
            ("Provide a textual description", "Select from predefined options"),
            help="Choose how you'd like to specify the complexity of your documents."
        )
        
        # Initialize session state for method and explanation
        if 'method' not in st.session_state:
            st.session_state.method = None
        
        if 'explanation' not in st.session_state:
            st.session_state.explanation = None
        
        if input_method == "Provide a textual description":
            complexity_description = st.text_area(
                "âœï¸ Describe the complexity and characteristics of your document:",
                help="Provide details such as the presence of images, diagrams, the structure's intricacy, etc."
            )
            
            complexity_description_submit = st.button("âœ… Submit Description", type="primary")
            
            if complexity_description_submit:
                # Check if the description is sufficiently detailed
                if len(complexity_description.strip()) > 5:
                    # Get processing method recommendation using the agent
                    with st.spinner("ðŸ”„ Analyzing your description to recommend a processing method..."):
                        try:
                            method, explanation = asyncio.run(process_method_recommendation(complexity_description))
                            st.session_state.method = method
                            st.session_state.explanation = explanation
                        except Exception as e:
                            logger.error(f"Error in method recommendation: {e}")
                            st.error(f"âŒ Failed to get method recommendation: {e}")
                else:
                    st.warning("âš ï¸ Please provide a more detailed description of your document complexity.")
        
        elif input_method == "Select from predefined options":
            complexity_option = st.selectbox(
                "ðŸ“Š Select document complexity:",
                list(PREDEFINED_RECOMMENDATIONS.keys()),
                help="This helps us choose the best processing method for your documents."
            )
            
            complexity_option_submit = st.button("âœ… Submit Selection", type="primary")
            
            if complexity_option_submit:
                # Get processing method recommendation based on the selected option
                with st.spinner("ðŸ”„ Determining the best processing method based on your selection..."):
                    try:
                        method, explanation = get_predefined_method_recommendation(complexity_option)
                        if method:
                            st.session_state.method = method
                            st.session_state.explanation = explanation
                        else:
                            st.error("âŒ No recommendation available for the selected option.")
                    except Exception as e:
                        logger.error(f"Error in predefined method recommendation: {e}")
                        st.error(f"âŒ Failed to get method recommendation: {e}")
        
        # After submission, display the recommendation if available
        if st.session_state.method and st.session_state.explanation:
            method_name = get_method_display_name(st.session_state.method)
            st.info(f"**âœ… Recommended Processing Method:** {method_name}")
            st.write(f"**ðŸ“ Reason:** {st.session_state.explanation}")
            
            # Add confirmation step
            st.write("### ðŸ”„ Confirm Processing")
            st.write(f"**ðŸ“„ Number of files to process:** {len(uploaded_files)}")
            for file in uploaded_files:
                st.write(f"- {file.name}")
            
            col1, col2 = st.columns(2)
            with col1:
                confirm = st.button("âœ… Confirm and Process Files", key="confirm_btn", type="primary")
            with col2:
                cancel = st.button("âŒ Cancel", key="cancel_btn", type="secondary")
            
            if confirm:
                try:
                    # Initialize config
                    config = ProcessingConfig(
                        azure_openai_key=st.secrets.get("AZURE_OPENAI_API_KEY"),
                        azure_endpoint=st.secrets.get("AZURE_OPENAI_ENDPOINT"),
                        openai_key=st.secrets.get("OPENAI_API_KEY"),
                        processing_method=st.session_state.method
                    )

                    # Create a dictionary to hold individual file statuses
                    file_status_mapping = {}
                    for file in uploaded_files:
                        file_status_mapping[file.name] = st.empty()
                        file_status_mapping[file.name].text(f"â³ Waiting to process {file.name}...")
                    
                    # Define a callback that process_document can use to update status
                    def update_file_status(filename: str, message: str, state: str = "running"):
                        """
                        Update the processing status for a specific file.
                        
                        Args:
                            filename (str): Name of the file being processed.
                            message (str): Status message to display.
                            state (str): State of the processing ('running', 'error', 'complete').
                        """
                        if filename in file_status_mapping:
                            # Prepend emojis based on state
                            emoji = "ðŸ”„" if state == "running" else "âŒ" if state == "error" else "âœ…"
                            file_status_mapping[filename].markdown(f"{emoji} {message}")

                    # Create tasks for processing each file
                    tasks = []
                    for file in uploaded_files:
                        file_content = file.read()
                        tasks.append(process_document(
                            file_data=file_content,
                            filename=file.name,
                            config=config,
                            sem=st.session_state.sem,
                            update_file_status_func=update_file_status
                        ))
                    
                    # Process files concurrently via asyncio.gather and run within asyncio's event loop
                    with st.spinner("ðŸ”„ Processing files..."):
                        processed_results = asyncio.run(asyncio.gather(*tasks, return_exceptions=True))
                    
                    # Handle results
                    success_count = 0
                    for file, result in zip(uploaded_files, processed_results):
                        if isinstance(result, Exception):
                            logger.error(f"Error processing {file.name}: {str(result)}")
                            update_file_status(file.name, f"âŒ Error: {str(result)}", state="error")
                        elif getattr(result, 'success', False) and getattr(result, 'document_info', None):
                            add_unified_document_chunk(
                                source_name=result.document_info.source_name,
                                index=result.document_info.index,
                                file_type=Path(file.name).suffix.lower().strip('.'),
                                text_chunk=result.document_info.text_chunk,
                                title=result.document_info.title,
                                hashtags=result.document_info.hashtags,
                                hypothetical_questions=result.document_info.hypothetical_questions,
                                summary=result.document_info.summary,
                                metadata=result.document_info.metadata
                            )
                            update_file_status(file.name, "âœ… Processing complete!", state="complete")
                            success_count += 1
                        else:
                            error_message = getattr(result, 'message', 'Unknown error.')
                            logger.error(f"Error processing {file.name}: {error_message}")
                            update_file_status(file.name, f"âŒ Error: {error_message}", state="error")
                    
                    # After processing all files, set up the search index
                    if success_count > 0:
                        with st.spinner("ðŸ” Setting up search index..."):
                            try:
                                index = asyncio.run(setup_hybrid_collection("hybrid_search", config))
                                if index:
                                    st.session_state['search_index'] = index
                                    st.success(f"ðŸŽ‰ Successfully processed {success_count}/{len(uploaded_files)} files.")
                                    
                                    # Enable search
                                    st.write("### ðŸ” Search Documents")
                                    query = st.text_input("ðŸ–‹ï¸ Enter your search query:")
                                    if query:
                                        with st.spinner("ðŸ”„ Executing search..."):
                                            results = asyncio.run(hybrid_search(query, "hybrid_search", index))
                                            if results:
                                                for doc in results:
                                                    st.write("---")
                                                    st.write(f"**ðŸ“„ Title:** {doc['title']}")
                                                    st.write(f"**ðŸ“ Summary:** {doc['summary']}")
                                                    st.write(f"**ðŸ“ Source:** {doc['source_name']}")
                                                    st.write(f"**ðŸ·ï¸ Tags:** {', '.join(doc['hashtags'])}")
                                                    st.write(f"**ðŸ—’ï¸ Text Chunk:** {doc['text_chunk']}")
                                            else:
                                                st.info("â„¹ï¸ No relevant documents found.")
                                else:
                                    st.error("âš ï¸ Failed to create search index.")
                            except Exception as e:
                                error_msg = f"âŒ Error setting up search index: {str(e)}"
                                logger.error(error_msg)
                                st.error(error_msg)
                    else:
                        st.error("âš ï¸ No files were successfully processed.")
                
                except Exception as e:
                    error_msg = f"âŒ Error during document processing: {str(e)}"
                    logger.error(error_msg)
                    st.error(error_msg)
            
            elif cancel:
                st.warning("ðŸš« File processing canceled.")
                st.stop()

    # ----------------------------------------------
    # Add Chat Input on the Sidebar
    # ----------------------------------------------
    st.sidebar.header("ðŸ’¬ Document Chat")

    # Initialize session state for chat history if not present
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    
    # Chat input - using st.chat_input if available, else fallback to st.text_input
    try:
        user_input = st.sidebar.chat_input("Ask a question about your documents:")
    except AttributeError:
        user_input = st.sidebar.text_input("Ask a question about your documents:")
    
    if user_input:
        with st.spinner("ðŸ” Searching for relevant information..."):
            if 'search_index' in st.session_state:
                try:
                    # Perform hybrid search with the user's query
                    results = asyncio.run(hybrid_search(user_input, "hybrid_search", st.session_state['search_index']))
                    
                    if results:
                        # Format the response
                        response = ""
                        for doc in results:
                            response += f"**ðŸ“„ Title:** {doc['title']}\n"
                            response += f"**ðŸ“ Summary:** {doc['summary']}\n"
                            response += f"**ðŸ“ Source:** {doc['source_name']}\n"
                            response += f"**ðŸ·ï¸ Tags:** {', '.join(doc['hashtags'])}\n"
                            response += f"**ðŸ—’ï¸ Text Chunk:** {doc['text_chunk']}\n\n"
                            response += "---\n\n"
                        # Append to chat history
                        st.session_state.chat_history.append(("User", user_input))
                        st.session_state.chat_history.append(("Bot", response))
                    else:
                        response = "â„¹ï¸ No relevant documents found for your query."
                        st.session_state.chat_history.append(("User", user_input))
                        st.session_state.chat_history.append(("Bot", response))
                except Exception as e:
                    logger.error(f"Error during chat search: {str(e)}")
                    response = "âŒ An error occurred while processing your request."
                    st.session_state.chat_history.append(("User", user_input))
                    st.session_state.chat_history.append(("Bot", response))
            else:
                response = "âš ï¸ No search index found. Please upload and process documents first."
                st.session_state.chat_history.append(("User", user_input))
                st.session_state.chat_history.append(("Bot", response))
    
    # Display chat history
    if st.session_state.chat_history:
        st.sidebar.markdown("### ðŸ—¨ï¸ Chat History")
        for sender, message in st.session_state.chat_history:
            if sender == "User":
                st.sidebar.markdown(f"**You:** {message}")
            else:
                st.sidebar.markdown(f"**Bot:** {message}")
    


# Initialize Streamlit UI
if __name__ == "__main__":
    display_file_upload_ui()
